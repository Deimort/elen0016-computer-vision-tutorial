{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple average filter (`cv2.blur()`) with `PyTorch`\n",
    "First we will implement an average filter using `PyTorch` with a `Conv2D` layer. As a reminder, a convolutional layer takes as inputs:\n",
    "* the number of channel at the input\n",
    "* the number of channel at the output\n",
    "* the kernel size\n",
    "* the stride\n",
    "* the padding\n",
    "* the dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../Images/boat.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(img, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unif_Blur(nn.Module):\n",
    "    def __init__(self, kernel):\n",
    "        super(Unif_Blur, self).__init__()\n",
    "\n",
    "        assert(kernel % 2 == 1)\n",
    "\n",
    "        padding = (kernel - 1)//2\n",
    "        \n",
    "        # out_dim = [(in_dim + 2*padding - dilation*(kernel-1) - 1)/stride] + 1\n",
    "        self.filter = nn.Conv2d(1, 1, kernel_size=kernel, padding=padding, padding_mode='reflect', bias=False)\n",
    "        \n",
    "        self.filter.weight = nn.Parameter((1/kernel**2)*torch.ones_like(self.filter.weight))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.filter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = Unif_Blur(5)\n",
    "\n",
    "img_tensor = torch.from_numpy(img)\n",
    "img_tensor = torch.unsqueeze(img_tensor, 0) # add channel dim\n",
    "img_tensor = torch.unsqueeze(img_tensor, 0) # add bs dim\n",
    "\n",
    "torch_blur = blur(img_tensor.float())\n",
    "torch_blur = np.reshape(torch_blur.data.numpy(), (512, 512))\n",
    "\n",
    "plt.imshow(torch_blur, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_blur = cv2.blur(img.astype('float64'), (5, 5))\n",
    "\n",
    "plt.imshow(cv_blur, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blured_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Classification is the most applied task in computer vision and is performed with convolutional neural networks (CNN). The basic idea for classification is to first extract features of an input image an then try to classify those features in order to predict the class of that image.  \n",
    "In this tutorial, we will apply image classification on the MNIST dataset (handwritten numbers).  \n",
    "The first task is to implement the `ConvNet` represented in the figure below.\n",
    "![ConvNet.png](images/convnet.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ClassNet, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the data loader\n",
    "We can retrieve the dataset from `PyTorch` and then define our `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = datasets.MNIST('data', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "test_mnist = datasets.MNIST('data', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "\n",
    "train_loader = DataLoader(train_mnist, batch_size=32, num_workers=2, shuffle=True)\n",
    "test_loader = DataLoader(test_mnist, batch_size=256, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the setup\n",
    "Here we have to define the `ConvNet` model that we will use. We also need to define our loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = ClassNet(10).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = pred.eq(y_true.view_as(pred)).sum()\n",
    "    accuracy = correct.float()/pred.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a training loop\n",
    "We can define a training loop where we pass sequentially the training data and we process the forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, device, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for iter_num, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        \n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = compute_accuracy(outputs, targets.to(device))\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "    print('train loss: {}, train accuracy:{}'.format(train_loss/len(train_loader), 100*train_acc/len(train_loader)))\n",
    "    \n",
    "def test(model, test_loader, device, critetion):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iter_num, (inputs, targets) in enumerate(test_loader):\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            loss = criterion(outputs, targets.to(device))        \n",
    "            test_loss += loss.item()\n",
    "\n",
    "            acc = compute_accuracy(outputs, targets.to(device))\n",
    "            test_acc += acc.item()\n",
    "        \n",
    "    print('test loss: {}, test accuracy:{}'.format(test_loss/len(test_loader), 100*test_acc/len(test_loader)))\n",
    "    \n",
    "def train(num_epoch=10):\n",
    "    for epoch in range(num_epoch):\n",
    "        print('epoch: {}/{}'.format(epoch+1, num_epoch))\n",
    "        train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "        test(model, test_loader, device, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection\n",
    "For object detection, the basic idea is the same as for the classification task. First we extract the features, then we perform a classification (to predict the classes) and a regression (to predict the bouding boxes).  \n",
    "For this tutorial we will use the `RetinaNet` architecture. It is composed of a backbone network (extract the features), a Feature Pyramid Network (merge the features of different pyramid layer) and 2 subnetworks (classification/regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "One fundamental concept with this architecture is the anchor. Anchors are pre-defined boxes with multiple scales and aspect ratios which are used as reference boxes. Anchors are assigned to a ground truth object’s box using an intersection-over-union (IoU) threshold of $0.5$, and to background if their IoU is in the interval $[0, 0.4)$.\n",
    "Most of the time, we define $\\sim100k$ anchors per images.  \n",
    "In the following figure, we can see some anchors with different scales and aspect ratios.  \n",
    "![anchors.png](images/anchors2.png)\n",
    "Finally the following figure shows how anchors (green) are assigned to grounf truths (red).\n",
    "![gt.png](images/anchor_box.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RetinaNet, Anchors, FocalLoss, BBoxTransform, ClipBoxes, inference, RandAugmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a `Dataset`\n",
    "For this example, we will use a custom dataset (cards) thus we will define a `Dataset class` to retrieve the data.  \n",
    "We will define a `Dataset`, a `Resizer`, a `Augmenter`, a `Normalizer` and an `UnNormalizer`.  \n",
    "Finally, we will define a `collater` given that our data have not the same number of object per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, annot, transform=None, ratio=None):\n",
    "        self.annot = annot\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_data, self.classes = self.read_annotations()\n",
    "        \n",
    "        self.classes = sorted(list(set(self.classes)))\n",
    "        \n",
    "        self.image_names = list(self.image_data.keys())\n",
    "        if ratio:\n",
    "            self.image_names = self.image_names[:round(len(self.image_names)*ratio)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.load_image(idx)\n",
    "        annotation = self.load_annotations(idx)\n",
    "        idx = self.image_names[idx]\n",
    "        scale = torch.ones((2))\n",
    "        \n",
    "        sample = {'image': image, 'annotation': annotation, 'idx': idx, 'scale': scale}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.classes)\n",
    "\n",
    "    def index_to_class(self, idx):\n",
    "        return self.classes[idx]\n",
    "\n",
    "    def label_to_index(self, label):\n",
    "        return self.classes.index(label)\n",
    "        \n",
    "    def load_annotations(self, index):\n",
    "        annotation_list = self.image_data[self.image_names[index]]\n",
    "        \n",
    "        annotations = torch.zeros((0, 5))\n",
    "        \n",
    "        if len(annotation_list) == 0:\n",
    "            return annotations\n",
    "        \n",
    "        for _, annot in enumerate(annotation_list):\n",
    "            x1 = int(annot['x1'])\n",
    "            y1 = int(annot['y1'])\n",
    "            x2 = int(annot['x2'])\n",
    "            y2 = int(annot['y2'])\n",
    "            \n",
    "            annotation = torch.zeros((1, 5))\n",
    "            \n",
    "            annotation[0, 0] = x1\n",
    "            annotation[0, 1] = y1\n",
    "            annotation[0, 2] = x2\n",
    "            annotation[0, 3] = y2\n",
    "            \n",
    "            annotation[0, 4] = int(self.classes.index(annot['label']))\n",
    "            \n",
    "            annotations = torch.cat((annotations, annotation), axis=0)\n",
    "            \n",
    "        return annotations\n",
    "        \n",
    "    def load_image(self, index):\n",
    "        image = Image.open(self.image_names[index]).convert('RGB')\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def read_annotations(self):\n",
    "        with open(self.annot, newline='') as csvfile:\n",
    "            f = pd.read_csv(csvfile, header=None)\n",
    "            \n",
    "            annotations = {}\n",
    "            classes = []\n",
    "            for index, row in f.iterrows():\n",
    "                image_name = row[0]\n",
    "                x1 = row[4]\n",
    "                y1 = row[5]\n",
    "                x2 = row[6]\n",
    "                y2 = row[7]\n",
    "                label = row[3].lower()\n",
    "                \n",
    "                classes.append(label)\n",
    "                                \n",
    "                if image_name not in annotations:\n",
    "                    annotations[image_name] = []\n",
    "                    \n",
    "                annotations[image_name].append({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2, 'label': label})\n",
    "        \n",
    "        return annotations, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resizer(object):\n",
    "    def __init__(self, dims=(224, 224)):\n",
    "        self.dims = dims\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['image'], sample['annotation']\n",
    "\n",
    "        w, h = image.size\n",
    "        \n",
    "        image = TF.resize(image, self.dims)\n",
    "        image = TF.to_tensor(image)\n",
    "\n",
    "        scale = torch.tensor([self.dims[0]/w, self.dims[1]/h])\n",
    "\n",
    "        annots[:, 0] *= scale[0]\n",
    "        annots[:, 1] *= scale[1]\n",
    "        annots[:, 2] *= scale[0]\n",
    "        annots[:, 3] *= scale[1]\n",
    "\n",
    "        sample['image'] = image\n",
    "        sample['annotation'] = annots\n",
    "        sample['scale'] = scale\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmenter(object):\n",
    "\n",
    "    def __init__(self, ra=False):\n",
    "\n",
    "        self.augment = RandAugmentation()\n",
    "        self.ra = ra\n",
    "\n",
    "    def __call__(self, sample, flip_x=0.5, prob_a=0.5):\n",
    "\n",
    "        image, annots = sample['image'], sample['annotation']\n",
    "\n",
    "        if self.ra:\n",
    "            if random.random() < prob_a:\n",
    "\n",
    "                image = TF.to_pil_image(image)\n",
    "\n",
    "                augment_img, method = self.augment(image)\n",
    "                \n",
    "                op = method['method']\n",
    "                val = method['value']\n",
    "\n",
    "                boxes = annots[:, :4].clone()\n",
    "\n",
    "                if op == 'rotate':\n",
    "                    boxes = augmentations.rotate_boxes(image, boxes, val)\n",
    "\n",
    "                if op == 'flip':\n",
    "                    boxes = augmentations.flip_boxes(image, boxes)\n",
    "\n",
    "                if op == 'mirror':\n",
    "                    boxes = augmentations.mirror_boxes(image, boxes)\n",
    "\n",
    "                new_boxes = torch.zeros((boxes.shape[0], 5))\n",
    "\n",
    "                new_boxes[:, :4] = boxes\n",
    "                new_boxes[:, 4] = annots[:, 4]\n",
    "\n",
    "                augment_img = TF.to_tensor(augment_img)\n",
    "\n",
    "                sample['image'] = augment_img\n",
    "                sample['annotation'] = new_boxes\n",
    "\n",
    "        else:\n",
    "            if random.random() < flip_x:\n",
    "\n",
    "                image = TF.to_pil_image(image)\n",
    "\n",
    "                image = TF.hflip(image)\n",
    "\n",
    "                w, h = image.size\n",
    "\n",
    "                image = TF.to_tensor(image)\n",
    "\n",
    "                x1 = annots[:, 0].clone()\n",
    "                x2 = annots[:, 2].clone()\n",
    "                \n",
    "                x_tmp = x1.clone()\n",
    "\n",
    "                annots[:, 0] = w - x2\n",
    "                annots[:, 2] = w - x_tmp\n",
    "\n",
    "                sample['image'] = image\n",
    "                sample['annotation'] = annots\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        image = TF.normalize(image, self.mean, self.std)\n",
    "\n",
    "        sample['image'] = image\n",
    "\n",
    "        return sample\n",
    "\n",
    "class UnNormalizer(object):\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collater(data):\n",
    "    images = [s['image'] for s in data]\n",
    "    annots = [s['annotation'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "    idxs = [s['idx'] for s in data]\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        annot_ = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        if max_num_annots > 0:\n",
    "            for idx, annot in enumerate(annots):\n",
    "                if annot.shape[0] > 0:\n",
    "                    annot_[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_ = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    return {'image': images, 'annotation': annot_, 'scale': scales, 'idx': idxs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_caption(image, box, caption):\n",
    "    b = np.array(box).astype(int)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "def draw_bb(data, classes):\n",
    "    image_tensor = data['image']\n",
    "    annotations = data['annotation']\n",
    "        \n",
    "    image = np.array(255*UnNormalizer()(image_tensor))\n",
    "    image = np.clip(image, 0, 255)\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    image = image.astype(np.uint8).copy()\n",
    "    \n",
    "    for i in range(annotations.shape[0]):\n",
    "        x1 = int(annotations[i, 0])\n",
    "        y1 = int(annotations[i, 1])\n",
    "        x2 = int(annotations[i, 2])\n",
    "        y2 = int(annotations[i, 3])\n",
    "        \n",
    "        image = cv2.rectangle(image, (x1, y1),(x2, y2), (0, 255, 0), 2)\n",
    "        draw_caption(image, (x1, y1, x2, y2), classes[int(annotations[i, 4])])\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CSVDataset('data/cards/train_labels.csv',\n",
    "                              transform=transforms.Compose([Resizer((512, 512)), Augmenter(), Normalizer()]))\n",
    "\n",
    "test_dataset = CSVDataset('data/cards/test_labels.csv',\n",
    "                             transform=transforms.Compose([Resizer((512, 512)), Normalizer()]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, num_workers=2, collate_fn=collater, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, num_workers=2, collate_fn=collater, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = FocalLoss(alpha=0.95, gamma=2, weight=1., device=device)\n",
    "anchors = Anchors(ratios=[0.625, 1.0, 1.6], scales=[0.803, 1.017, 1.312], device=device)\n",
    "regressBoxes = BBoxTransform()\n",
    "clipBoxes = ClipBoxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetinaNet('resnet101', num_classes=len(train_dataset.classes), pretrained=True)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    print('epoch: {}/{}'.format(i+1, num_epochs))\n",
    "    model.train()\n",
    "\n",
    "    class_loss = []\n",
    "    reg_loss = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for iter_num, data in enumerate(train_loader):\n",
    "        imgs = data['image'].float().to(device)\n",
    "        classification, regression = model(imgs)\n",
    "        anchrs = anchors(imgs)\n",
    "        classification_loss, regression_loss = criterion(classification, regression, anchrs, data['annotation'].to(device))\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "\n",
    "        loss = classification_loss + regression_loss\n",
    "\n",
    "        class_loss.append(classification_loss.item())\n",
    "        reg_loss.append(regression_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    classification_loss = np.mean(class_loss)\n",
    "    regression_loss = np.mean(reg_loss)\n",
    "    loss = classification_loss + regression_loss\n",
    "    print('Training loss: {}'.format(loss))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    class_loss = []\n",
    "    reg_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter_num, data in enumerate(test_loader):\n",
    "\n",
    "            imgs = data['image'].float().to(device)\n",
    "\n",
    "            classification, regression = model(imgs)\n",
    "            anchrs = anchors(imgs)\n",
    "            classification_loss, regression_loss = criterion(classification, regression, anchrs, data['annotation'].to(device))\n",
    "\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            loss = classification_loss + regression_loss\n",
    "\n",
    "            class_loss.append(classification_loss.item())\n",
    "            reg_loss.append(regression_loss.item())\n",
    "\n",
    "    classification_loss = np.mean(class_loss)\n",
    "    regression_loss = np.mean(reg_loss)\n",
    "    loss = classification_loss + regression_loss\n",
    "    print('Validation loss: {}'.format(loss))\n",
    "    scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_loader = DataLoader(test_dataset, num_workers=1, collate_fn=collater, batch_size=1, shuffle=False)\n",
    "iter_loader = iter(predict_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "data = next(iter_loader)\n",
    "with torch.no_grad():\n",
    "\n",
    "    img = data['image'][0].float().to(device).unsqueeze(dim=0)\n",
    "    classification, regression = model(img)\n",
    "    anchrs = anchors(img)\n",
    "\n",
    "    scores, labels, boxes = inference(classification, regression, img, anchrs, regressBoxes, clipBoxes, device, score_threshold=0.05, nms_threshold=0.2)\n",
    "\n",
    "    idxs = np.where(scores.cpu()>=0.2)\n",
    "    boxes = boxes[idxs]\n",
    "    labels = labels[idxs].view((boxes.shape[0], 1))\n",
    "\n",
    "    annotations = torch.cat((boxes, labels), 1)\n",
    "\n",
    "    draw_bb({'image': data['image'][0], 'annotation': annotations}, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
